{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Auth\n",
    "// 1,C625$@DOM1,U147@DOM1,C625,C625,Negotiate,Batch,LogOn,Success\n",
    "// 1,C653$@DOM1,SYSTEM@C653,C653,C653,Negotiate,Service,LogOn,Success\n",
    "// 1,U620$@DOM1,SYSTEM@C660,U620,C660,Negotiate,Service,LogOn,Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Red Team\n",
    "// 150885,U620@DOM1,C17693,C620\n",
    "// 151036,U748@DOM1,C17693,C305\n",
    "// 151648,U748@DOM1,C17693,C728\n",
    "// 151993,U6115@DOM1,C17693,C1173\n",
    "// 153792,U636@DOM1,C17693,C294\n",
    "// 155219,U748@DOM1,C17693,C5693\n",
    "// 155399,U748@DOM1,C17693,C152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Shariq:4041\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = local[*], app id = local-1586080917058)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat\r\n",
       "import org.apache.hadoop.io.LongWritable\r\n",
       "import org.apache.hadoop.io.Text\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n",
    "import org.apache.hadoop.io.LongWritable\n",
    "import org.apache.hadoop.io.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"textinputformat.record.delimiter\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redTeam: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = C:/Users/welcome/Documents/Vishal/redteam.txt NewHadoopRDD[295] at newAPIHadoopFile at <console>:51\n"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val redTeam = sc.newAPIHadoopFile(\"C:/Users/welcome/Documents/Vishal/redteam.txt\",\n",
    "                                   classOf[TextInputFormat], classOf[LongWritable],classOf[Text],conf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extracted: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[296] at map at <console>:53\n"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val extracted = redTeam.map(ele => ele._2.toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redTeam: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[356] at map at <console>:58\n"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val redTeam = extracted.map { r =>\n",
    "  val t = r.split(\",\")\n",
    "  (t(0).trim(),t(1).trim(),t(2).trim(),t(3).trim())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "redTeamDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newNames: Seq[String] = List(id, x1, x2, x3)\r\n",
       "redTeamDF: org.apache.spark.sql.DataFrame = [id: string, x1: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newNames = Seq(\"id\", \"x1\", \"x2\", \"x3\")\n",
    "val redTeamDF = redTeam.toDF(newNames: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----+\n",
      "|    id|        x1|    x2|   x3|\n",
      "+------+----------+------+-----+\n",
      "|150885| U620@DOM1|C17693| U620|\n",
      "|151036| U748@DOM1|C17693| C305|\n",
      "|151648| U748@DOM1|C17693| C728|\n",
      "|151993|U6115@DOM1|C17693|C1173|\n",
      "|153792| U636@DOM1|C17693| C294|\n",
      "+------+----------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "redTeamDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auth: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = C:/Users/welcome/Documents/Vishal/auth.txt NewHadoopRDD[302] at newAPIHadoopFile at <console>:51\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val auth = sc.newAPIHadoopFile(\"C:/Users/welcome/Documents/Vishal/auth.txt\",\n",
    "                                   classOf[TextInputFormat], classOf[LongWritable],classOf[Text],conf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extractedAuth: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[303] at map at <console>:53\n"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val extractedAuth = auth.map(ele => ele._2.toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auth: org.apache.spark.rdd.RDD[(String, String, String, String, String, String, String, String, String)] = MapPartitionsRDD[361] at map at <console>:58\n"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val auth = extractedAuth.map { r =>\n",
    "  val t = r.split(\",\")\n",
    "  (t(0).trim(),t(1).trim(),t(2).trim(),t(3).trim(),t(4).trim(),t(5).trim(),t(6).trim(),t(7).trim(),t(8).trim())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newNamesAuth: Seq[String] = List(id, x1, x2, x3, x4, x5, x6, x7, x8)\r\n",
       "authDF: org.apache.spark.sql.DataFrame = [id: string, x1: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newNamesAuth = Seq(\"id\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\")\n",
    "val authDF = auth.toDF(newNamesAuth: _*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----+----+---------+-------+-----+-------+\n",
      "| id|       x1|         x2|  x3|  x4|       x5|     x6|   x7|     x8|\n",
      "+---+---------+-----------+----+----+---------+-------+-----+-------+\n",
      "|  1|C625@DOM1|  U147@DOM1|C625|C625|Negotiate|  Batch|LogOn|Success|\n",
      "|  1|C653@DOM1|SYSTEM@C653|C653|C653|Negotiate|Service|LogOn|Success|\n",
      "|  1|U620@DOM1|SYSTEM@C620|U620|U620|Negotiate|Service|LogOn|Success|\n",
      "+---+---------+-----------+----+----+---------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----+\n",
      "|    id|        x1|    x2|   x3|\n",
      "+------+----------+------+-----+\n",
      "|150885| U620@DOM1|C17693| U620|\n",
      "|151036| U748@DOM1|C17693| C305|\n",
      "|151648| U748@DOM1|C17693| C728|\n",
      "|151993|U6115@DOM1|C17693|C1173|\n",
      "|153792| U636@DOM1|C17693| C294|\n",
      "+------+----------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "redTeamDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "import org.apache.spark.sql.functions._\r\n",
       "redTeamList: org.apache.spark.broadcast.Broadcast[String] = Broadcast(109)\n"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "//collect all the srdId and dstId from and broadcast\n",
    "val redTeamList = sc.broadcast(redTeamDF.select(collect_set(\"x3\").as(\"sourceDomain\")).rdd.map(row => row.getAs[Seq[String]](0) toString).collect()(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res183: String = WrappedArray(C4280, C1173, C152, C1493, C294, C231, C721, C305, C1567, C332, C2341, U620, C728, C504, C148, C1003, C5693)\n"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redTeamList.value.toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----+----+---------+-------+-----+-------+\n",
      "| id|       x1|         x2|  x3|  x4|       x5|     x6|   x7|     x8|\n",
      "+---+---------+-----------+----+----+---------+-------+-----+-------+\n",
      "|  1|C625@DOM1|  U147@DOM1|C625|C625|Negotiate|  Batch|LogOn|Success|\n",
      "|  1|C653@DOM1|SYSTEM@C653|C653|C653|Negotiate|Service|LogOn|Success|\n",
      "|  1|U620@DOM1|SYSTEM@C620|U620|U620|Negotiate|Service|LogOn|Success|\n",
      "+---+---------+-----------+----+----+---------+-------+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filter_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, x1: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filter_df = authDF.where((authDF(\"x4\") !== redTeamList.value))\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----+----+---------+-------+-----+-------+------------------+\n",
      "| id|       x1|         x2|  x3|  x4|       x5|     x6|   x7|     x8|Malicious Activity|\n",
      "+---+---------+-----------+----+----+---------+-------+-----+-------+------------------+\n",
      "|  1|C625@DOM1|  U147@DOM1|C625|C625|Negotiate|  Batch|LogOn|Success|             false|\n",
      "|  1|C653@DOM1|SYSTEM@C653|C653|C653|Negotiate|Service|LogOn|Success|             false|\n",
      "|  1|U620@DOM1|SYSTEM@C620|U620|U620|Negotiate|Service|LogOn|Success|              true|\n",
      "+---+---------+-----------+----+----+---------+-------+-----+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "checkActivity: (words: org.apache.spark.broadcast.Broadcast[String])org.apache.spark.sql.expressions.UserDefinedFunction\n"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// redteam file i kept one user i.e. U620 and the same is available in auth file, In this setting up the flag from \n",
    "// Auth dataset, if it's avilable with redTeam list (which indicate it's malicioius activity with that user id)\n",
    "def checkActivity(words: org.apache.spark.broadcast.Broadcast[String]) = {\n",
    "  udf {(s: String) => words.value.contains(s.toString)}\n",
    "}\n",
    "\n",
    "authDF.withColumn(\"Malicious Activity\", checkActivity(redTeamList)($\"x4\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.rdd.MapPartitionsRDD\n",
      "file:/C:/Users/welcome/Documents/Vishal/redteam.txt===>150885,U620@DOM1,C17693,U620\n",
      "151036,U748@DOM1,C17693,C305\n",
      "151648,U748@DOM1,C17693,C728\n",
      "151993,U6115@DOM1,C17693,C1173\n",
      "153792,U636@DOM1,C17693,C294\n",
      "155219,U748@DOM1,C17693,C5693\n",
      "155399,U748@DOM1,C17693,C152\n",
      "155460,U748@DOM1,C17693,C2341\n",
      "155591,U748@DOM1,C17693,C332\n",
      "156658,U748@DOM1,C17693,C4280\n",
      "210086,U748@DOM1,C18025,C1493\n",
      "210294,U748@DOM1,C18025,C1493\n",
      "210312,U748@DOM1,C18025,C1493\n",
      "218418,U748@DOM1,C17693,C504\n",
      "227052,U748@DOM1,C17693,C148\n",
      "227408,U748@DOM1,C17693,C148\n",
      "227520,U748@DOM1,C17693,C148\n",
      "227780,U748@DOM1,C17693,C148\n",
      "228024,U748@DOM1,C17693,C148\n",
      "228150,U748@DOM1,C17693,C148\n",
      "228642,U1723@DOM1,C17693,C231\n",
      "228658,U1723@DOM1,C17693,C231\n",
      "229046,U1723@DOM1,C17693,C231\n",
      "230395,U1723@DOM1,C17693,C1003\n",
      "234667,U748@DOM1,C17693,C504\n",
      "460197,U748@DOM1,C17693,C721\n",
      "460711,U748@DOM1,C17693,C721\n",
      "460991,U748@DOM1,C17693,C721\n",
      "461842,U748@DOM1,C17693,C1567\n",
      "463374,U748@DOM1,C17693,C1003\n",
      "464246,U748@DOM1,C17693,C1567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, String)] = C:/Users/welcome/Documents/Vishal/redteam.txt MapPartitionsRDD[426] at wholeTextFiles at <console>:57\n"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = spark.sparkContext.wholeTextFiles(\"C:/Users/welcome/Documents/Vishal/redteam.txt\")\n",
    "println(rdd.getClass)\n",
    "  rdd.foreach(f=>{\n",
    "    println(f._1+\"===>\"+f._2)\n",
    "  })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
